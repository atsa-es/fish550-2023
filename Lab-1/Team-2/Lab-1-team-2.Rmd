# Lab 1: ARIMA models

Team member names: Eric French (Civil), Dylan Hubl (ESRM), Miranda Mudge (Molecular & Cell Bio)

# Data

WE will be working with the Bristol Bay data set. Our focus will be on 4-year-old Sockeye in the Wood, Kvichak, and Ugashik regions.

```{r}
library(tidyverse)
library(ggplot2)
library(forecast)
library(zoo)

bb_data <- readRDS(here::here("Lab-1", "Data_Images", "bristol_bay_data_plus_covariates.rds"))
bb_data$log_ret <- log(bb_data$ret)
bb_data$full_age <- bb_data$fw_age + bb_data$o_age
```

# Question your team will address

Our group decided to compare how accurate the forecasts of best fit ARIMA models were for the populations of 4 year old salmon (1.3 and 2.2 age groups) were in the Wood, Kvichak, and Ugashik regions of the Bristol Bay data. Our questions were the following:

1.  What kind of ARIMA model best fits the population in each chosen region and age group of sockeye salmon?

2.  Which populations of salmon can be better forecast using ARIMA modeling? Those that have spent 2 years in the ocean or 3?

3.  Is there regional variation in the population changes of 4-year-old salmon?

# Method you will use

## Initial plan

Using the forcast package, we plan to fit ARIMA models to the 1960-2010 data on 4 yr old fish in the Wood and Ugashik systems. Then forecast to 2020. We will then split up the 1.3 and 2.2 age groups and compare. We will measure accuracy by the comparing the RMS error of each model.

## What you actually did

We were able to enact our plan with a few modifications. Instead of modeling all the 4 year old salmon together, we separated and compared them by age group from the outset. Additionally, we added the Kvichak region to our analysis. We noticed there was an outlier in the data for the 1yr freshwater and 3yr ocean fish in the Ugashik region, so we fit a second model to the data starting at the year 1980 and compared its forecast to the original model's.

# Diagnostics and preliminary exploration

## Plot the data
```{r}
# Miranda's Code
age1.3 = 1.3
age2.2 = 2.2

dat_1.3 <- bb_data %>%
  filter(system==region) %>%
  filter(age_group == age1.3) %>%
  mutate(lnreturns = log(ret), #returns same values as code above
         year = ret_yr) %>%
  select(year, lnreturns) 

dat_2.2 <- bb_data %>%
  filter(system==region) %>%
  filter(age_group == age2.2) %>%
  mutate(lnreturns = log(ret), #returns same values as code above
         year = ret_yr) %>%
  select(year, lnreturns)


datts_1.3 <- ts(dat_1.3$lnreturns, start=dat_1.3$year[1]) # time series 1.3 fish 
datts_1.3_out <- window(datts_1.3, start=1980, end = 2020) # time series 1.3 fish 
datts_2.2 <- ts(dat_2.2$lnreturns, start=dat_2.2$year[1]) # time series 2.2 fish

plot.ts(datts_1.3, ylab = "Log abundance", main = "Time series of fish: 1 year freshwater, 3 years ocean") 
plot.ts(datts_1.3_out, ylab = "Log abundance", main = "Time series of fish: 1 year freshwater, 3 years ocean") 
plot.ts(datts_2.2, ylab = "Log abundance", main = "Time series of fish: 2 year freshwater, 2 years ocean") 

#Dylan's Code
wood_dt <- bb_data[(bb_data$system == rivers[2]) & (bb_data$full_age == 4),]
unique(wood_dt$o_age)
#split the dataframe by years in the ocean
wood3 <- wood_dt[wood_dt$o_age==3,]
wood2 <- wood_dt[wood_dt$o_age==2,]

#make them time series
wood3.ts <- ts(wood3$log_ret, start = 1963, frequency = 1)
wood2.ts <- ts(wood2$log_ret, start = 1963, frequency = 1)

#check the plot
plot.ts(wood2.ts, ylab = "log(counted fish x1000)", main ="4 year old sockeye returning to Wood River\nthat spent 2 years in Ocean" )
#looks like it could potentially be stationary
plot.ts(wood3.ts, ylab = "log(fish counted x1000)", main ="4 year old sockeye returning to Wood River\nthat spent 3 years in Ocean" )
#looks like there is a positive trend to the data: not stationary, or could be stationary around a trend

Kvichak_dt <- bb_data[(bb_data$system == rivers[4]) & (bb_data$full_age == 4),]
unique(Kvichak_dt$o_age)
kvi.3 <- Kvichak_dt[Kvichak_dt$o_age == 3,]
kvi.2 <- Kvichak_dt[Kvichak_dt$o_age == 2,]

#make the ts
kvi.3.ts <- ts(kvi.3$log_ret, start = 1963, end = 2020)
kvi.2.ts <- ts(kvi.2$log_ret, start = 1963, end = 2020)

#plots
layout(layout_mat_2)
plot(kvi.2.ts, ylab = "log(fish counted x1000)", main = "4 year old sockey returning to Kvichak River\nthat spent 2 years in Ocean" )
#looks fairly stationary to me
plot(kvi.3.ts, ylab = "log(fish counted x1000)", main = "4 year old sockeye returning to Kvichak River\nthat spent 3 years in Ocean" )
#one huge spike downward at start otherwise variance looks similar througout. maybe stationary around a trend if the initial spike doesn't ruin it
```

Plot the data and discuss any obvious problems with stationarity from your visual test.Side

## Use ACF, PACF, and decomposition
```{r}
#Miranda's Code
acf(datts_1.3)
acf(datts_1.3_out, na.action = na.pass) # allows na value
acf(datts_2.2)

pacf(datts_1.3)
pacf(datts_1.3_out,na.action = na.pass)
pacf(datts_2.2)
datts_decomp <- decompose(datts) #Apply this to all variables
#Dylan's Code
#acf and pacf for differenced wood3
layout_matrix_1 <- matrix(c(1,2),ncol=2)  #setting up how the plots will display
layout(layout_matrix_1)                   #setting up how the plots will display
acf(diff(wood3.ts))
pacf(diff(wood3.ts))
#pacf looks like a slow decay indicating it will be and MA model. signif lags at 1, 5, 10 on ACF

#acf and pacf for wood2
#carries the plot display set up from the above over to these plots
acf(wood2.ts)
pacf(wood2.ts)
dev.off()        #resets the plot window so it should only show a single plot at a time
#no significant lags in either plot. This is white noise, no autocorrelation. expecting ARIMA(0,0,0)

#plot acf and pacf
layout(layout_matrix_1)
acf(kvi.2.ts)
pacf(kvi.2.ts)
#neither shows any clear lag or decay signs. maybe an combined ARMA model

acf(kvi.3.ts)
pacf(kvi.3.ts)
#both have a significant lag at 1. maybe just a differencing will help
acf(diff(kvi.3.ts))
pacf(diff(kvi.3.ts))
#looks like slow decay on pacf and a significant lag at 1 on acf. expecting an ARIMA(0,1,1)
```
Use the ACF and PACF plots to explore the time series. What did you learn? Also try decomposition.
```{r} 
datts_decomp <- decompose(datts) #Apply this to all variables
```

## Test for stationarity

```{r}
#Miranda's Code
tseries::adf.test(datts_1.3) 
tseries::kpss.test(datts_1.3, null = "Level") 
tseries::kpss.test(datts_1.3, null = "Trend") 

tseries::adf.test(datts_2.2) 
tseries::kpss.test(datts_2.2, null = "Level") 
tseries::kpss.test(datts_2.2, null = "Trend")

datts_diff_1.3 <- diff(datts_1.3, differences = 1)
datts_diff_1.3_out <- diff(datts_1.3_out, differences = 1)
datts_diff_2.2 <- diff(datts_2.2, differences = 1)

plot(datts_diff_1.3, ylab = "Log abundance", main = "Time series, first differenced: 1 year freshwater, 3 years ocean")
plot(datts_diff_1.3_out, ylab = "Log abundance", main = "Time series, first differenced: 1 year freshwater, 3 years ocean, outlier removed")
plot(datts_diff_2.2, ylab = "Log abundance", main = "Time series, first differenced: 2 year freshwater, 2 years ocean")

#Dylan's Code
#wood3.ts testing
#run tests for stationarity
tseries::adf.test(wood3.ts)                  # fails to reject: Non-stationary
tseries::adf.test(wood3.ts,k = 0)            #forced it to test AR(1) it does reject: Stationary. So conflicts with the test above
tseries::kpss.test(wood3.ts, null = "Level") #not sure this test makes sense, visually there is a trend in the data. opposite null hypothesis for kpss it rejects: Non-stationary
tseries::kpss.test(wood3.ts, null = "Trend") #this tested to see if data is stationary around a trend
# just barely rejects stationarity: meaning it is non-stationary
#wood3 has disagreement on stationairity, so we need to fix that by differencing the data
#figure out how many differences you need to reach stationairity
forecast::ndiffs(wood3.ts, test = "kpss")    # 1 difference needed
forecast::ndiffs(wood3.ts, test = "adf")     # 0 differences needed, again disagreement. Likely will need a difference but we can compare to the ARIMA model that gets automatically fit

#wood2.ts testing
tseries::adf.test(wood2.ts)                  # Rejects: stationary
tseries::adf.test(wood2.ts,k = 0)            #forced it to test AR(1) it does reject: Stationary
tseries::kpss.test(wood2.ts, null = "Level") #opposite null hypothesis for kpss it fails to reject: Stationary
tseries::kpss.test(wood2.ts, null = "Trend") #not sure that this test makes sense (talked to Mark about this he said it can still be good to check as the trend could be very subtle), visually data does not appear to have a trend.it fails to reject: stationary
#wood2 has full agreement on stationairity from all tests used. Very likely is stationary and will not require differencing
#see if ndiffs says 0, i expect it to
forecast::ndiffs(wood2.ts, test = "kpss")    # 0 needed
forecast::ndiffs(wood2.ts, test = "adf")     # 0 needed. Agrees with the unit root tests above that found stationairity of the data

#test stationairity kvi.2
tseries::adf.test(kvi.2.ts)                  # fails to Reject: non-stationary
tseries::adf.test(kvi.2.ts,k = 0)            #forced it to test AR(1) it does reject: Stationary. disagreement between them
tseries::kpss.test(kvi.2.ts, null = "Level") #opposite null hypothesis for kpss it fails to reject: Stationary
tseries::kpss.test(kvi.2.ts, null = "Trend") #not sure that this test makes sense (talked to Mark about this he said it can still be good to check as the trend could be very subtle), visually data does not appear to have a trend.it fails to reject: stationary
#there is disagreement from adf and kpss, may require differencing

forecast::ndiffs(kvi.2.ts, test = "kpss")    # 0 needed
forecast::ndiffs(kvi.2.ts, test = "adf")     # 0 needed. So I am expecting this to be an AR(1) as the full adf test was the only one to say non-stationary

#test stationairity kvi.3
tseries::adf.test(kvi.3.ts)                  #  Rejects just barely: stationary
tseries::adf.test(kvi.3.ts,k = 0)            #forced it to test AR(1) it does reject: Stationary.
tseries::kpss.test(kvi.3.ts, null = "Level") #opposite null hypothesis for kpss it  rejects: non-Stationary
tseries::kpss.test(kvi.3.ts, null = "Trend") #.it rejects:non- stationary

forecast::ndiffs(kvi.3.ts, test = "kpss")    # 1 needed
forecast::ndiffs(kvi.3.ts, test = "adf")     #0 needed. again disagreement. likely will be differenced by auto.arima()
#there is disagreement from adf and kpss, may require differencing
```
Run tests and discuss any stationarity issues and how these were addressed.

Fitting entire datasets with auto.arima
```{r}
#Only Dylan's code here. Replicate for Ugashik or get rid of it?
#auto fit the models to data
forecast::auto.arima(diff(wood3.ts)) #MA(1) only not sure what to make of the lags at 5 and 10. could be the cyclic nature of the fish Eli has mentioned (see note further down)
forecast::auto.arima(wood3.ts)       # is in agreement with need for differencing and MA(1). I think "with drift" in the non-differenced data is the equivalent of "with non-zero mean" in the pre-differenced model above.
forecast::auto.arima(wood2.ts)       #my expectation panned out that this is just white noise but has non-zero mean which makes sense as this is count data
#plot of our models
layout_mat_2 <- matrix(c(1,1,2,2), nrow =2, byrow = TRUE)
layout(layout_mat_2)
plot.ts(wood2.ts, main ="4 year old sockeye returning to Wood River\nthat spent 2 years in Ocean", ylab = "log(counted fish x1000")
lines(fitted(fit.wood2), col ="red")
plot.ts(wood3.ts, main ="4 year old sockeye returning to Wood River\nthat spent 3 years in Ocean", ylab = "log(counted fish x1000")
lines(fitted(fit.wood3), col ="red")
forecast::checkresiduals(fit.wood3)  #All good news. fail to reject null hypothesis of Ljung-Box, which means the residuals are not autocorrelated. the ACF plot agrees with no significant lags and the histogram looks like a normal distribution
forecast::checkresiduals(fit.wood2)  #All good news again. fail to reject null of Ljung-Box. no significant lags on ACF. Histogram looks pretty good, two large negative values but otherwise good.


#fit models
fit.kvi.2 <- forecast::auto.arima(kvi.2.ts)
#ARIMA(0,0,1) with non-zero mean. significant pacf lag at 7 could be cyclic nature of date obscuring the MA() pattern
fit.kvi.3 <- forecast::auto.arima(kvi.3.ts)
#ARIMA(0,1,1) as expected from acf and pacf plots
#plot
dev.off()
layout(layout_mat_2)
plot(kvi.2.ts, main= "4 year old sockeye returning to Kvichak River\nthat spent 2 years in ocean", ylab = "log(count of fish x1000)")
lines(fitted(fit.kvi.2), col = "red")
plot(kvi.3.ts, main= "4 year old sockeye returning to Kvichak River\nthat spent 3 years in ocean", ylab = "log(count of fish x1000)")
lines(fitted(fit.kvi.3), col= "red")
#check residuals
forecast::checkresiduals(fit.kvi.2)
#here we see the that the residuals are autocorrelated as Eli had mentioned
#we might find. we fail to reject Ljung-Box telling us the same thing. Histogram looks
#good. Just mention we see the autocorrelation
forecast::checkresiduals(fit.kvi.3)
#this one does much better. fails to reject Ljung-Box and acf plot both telling us no autocorrelation. 
#histogram looks ok. we see the giant negative spike in the tail. lots of variance in the first few years
```

Fit Models to the training data
```{r}
#Miranda's Code
# Make dataframes for testing models later
train_1.3 <- window(datts_1.3, 1963, 2010) # range 1963 - 2010
test_1.3 <- window(datts_1.3, 2011, 2020) # range 2011 - 2020

train_1.3_out <- window(datts_1.3_out, 1980, 2010) # range 1963 - 2010
test_1.3_out <- window(datts_1.3_out, 2011, 2020) # range 2011 - 2020

train_2.2 <- window(datts_2.2, 1963, 2010) # range 1963 - 2010
test_2.2 <- window(datts_2.2, 2011, 2020) # range 2011 - 2020

model_1.3 <- auto.arima(train_1.3)
model_1.3

model_1.3_out <- auto.arima(train_1.3_out)
model_1.3_out

model_2.2 <- auto.arima(train_2.2)
model_2.2

#Dylan's Code, need to speak on auto.arima model results
#creating the training and testing periods for models
train.wood2 <- window(wood2.ts, start = 1963, end = 2010)
test.wood2 <- window(wood2.ts, start = 2011, end = 2020)
fit.train <- auto.arima(train.wood2)

train.wood3 <- window(wood3.ts, start = 1963, end = 2010)
test.wood3 <- window(wood3.ts, start = 2011, end = 2020)
fit.train3 <- auto.arima(train.wood3)

#train and test predictions
train.kvi.2 <- window(kvi.2.ts, start= 1963, end=2010)
test.kvi.2 <- window(kvi.2.ts, start= 2011, end= 2020)
fit.train.k2 <- forecast::auto.arima(train.kvi.2)

train.kvi.3 <- window(kvi.3.ts, start= 1963, end=2010)
test.kvi.3 <- window(kvi.3.ts, start= 2011, end= 2020) 
fit.train.k3 <- forecast::auto.arima(train.kvi.3)
```

### Printed results for ret_yr 1.3

Series: train ARIMA(0,1,1)

Coefficients: ma1 -0.7305 s.e. 0.1400

sigma\^2 = 18.34: log likelihood = -134.93 AIC=273.86 AICc=274.13
BIC=277.56

Series: train_1.3 ARIMA(2,1,2)

Coefficients: ar1 ar2 ma1 ma2 0.6167 -0.9676 -0.8030 0.8451 s.e. 0.0500
0.0341 0.1359 0.1212

sigma\^2 = 1.161: log likelihood = -68.56 AIC=147.11 AICc=148.58
BIC=156.36

ARIMA model results from train data for ret_yr 1.3 fish in Bristol Bay
very different from ret_yr 2.2 despite both being 4 yr old fish. Ret_yr
1.3 inconsistent with results of ACF and PACF tests suggesting AR(1).
First differencing the time series object returned the mean to zero, and
auto.arima agrees first differencing the data is the best approach for
model.

Unfortunately, this is totally different from the stationarity tests
which suggest that that the data is stationary and the AICc is huge
which suggests this is not the best model for this data. Below is the
better model.

### Printed results for ret_yr 1.3 outlier removed

Series: train_1.3_out ARIMA(2,0,0) with non-zero mean

Coefficients: ar1 ar2 mean 0.3818 -0.5662 6.9256 s.e. 0.1506 0.1451
0.0890

sigma\^2 = 0.365: log likelihood = -27.2 AIC=62.41 AICc=63.94 BIC=68.14

By focusing on years without the outlier value, the model changed to
ARIMA (2,0,0) and the AICc is much lower, suggesting this model is the
better fit for the data. For the 1 year freshwater - 3 year ocean fish
we will proceed with this model.

### Printed results for ret_yr 2.2

Series: train ARIMA(1,0,0) with non-zero mean

Coefficients: ar1 mean 0.4545 5.9958 s.e. 0.1268 0.3373

sigma\^2 = 1.753: log likelihood = -80.67 AIC=167.34 AICc=167.88
BIC=172.95

ARIMA model results from train data for ret_yr 2.2 fish in Bristol Bay
consistent with results of ACF and PACF tests suggesting AR(1). First
differencing the time series object returned the mean to zero, but
auto.arima suggests 0 differencing and that data has non-zero mean.

# Results
Plotting Best Fit Results

Ugashik 1.3, Starting from 1980
```{r}
#Miranda's Code
fit_1.3 <- auto.arima(train_1.3_out) #fit the model
fc_1.3 <- forecast(fit_1.3, h=10)

fr_1.3 <- accuracy(forecast(fit_1.3, h=10), test_1.3_out) #test accuracy

plot.ts(datts_1.3) # check fit
lines(fitted(fit_1.3), col="red") #model seems to fit well

forecast::checkresiduals(fit_1.3) # looks better than 1.3 trained on 1960-2010

autoplot(fc_1.3) + 
  geom_point(aes(x=x, y=y), data=fortify(test_1.3_out)) 
```
### Results for 1.3 fish in Ugashik

The model itself seems to fit the data well based on the visual fit and
the residuals.

The model forecast centers around the mean of the train data, predicting
that 2010 will drop which fits the first few actual data points well,
but doesn't do as good of a job of predicting that the data from
2015-2020 look closer to the more recent years than further out (prior
to 2005). Data are included in the confidence interval though which is
great.

Ugashik 2.2
```{r}
fit_2.2 <- auto.arima(train_2.2) #fit the model
fc_2.2 <- forecast(fit_2.2, h=10)

fr_2.2 <- accuracy(forecast(fit_2.2, h=10), test_2.2) #test accuracy

plot.ts(datts_2.2) # check fit
lines(fitted(fit_2.2), col="red") #model seems to fit well, follows overall trend as it changes

forecast::checkresiduals(fit_2.2) 

autoplot(fc_2.2) + 
  geom_point(aes(x=x, y=y), data=fortify(test_2.2)) 
```
### Results for 2.2 fish in Ugashik

The model fits the timeseries train data pretty well, matching the
overall change in curve. The residuals show the ACF plot has 1
significant lag reflecting the ARIMA (1,0,0) model, the histogram is
fairly centered as expected, and model fails to reject Ljung-Box test.

Model forecast doesn't look great. The forecast flatlines while the real
data from 2011-2020 have much higher variance. This reflects the high
AICc from the model. But the data are contained within the predicted
confidence interval which is good.

Wood 1.3
```{r}
pred.wood3 <- forecast(fit.train3, h = 10)
plot(pred.wood3,ylab = "log(fish counted x1000)", xlab = "Time")
points(test.wood3, pch=19, col="red")
#we see that the prediction intervals widen as the forecast moves forward
#likely due to the autocorrelation of the errors, because this is an MA(1) model,
#which the model has to estimate with each successive prediction. The fitted points
#are a flat line again because this is an MA(1) model, we would write the model as
# Xt = Xt-1 + error. When we fitted a model to the entire dataset, auto.arima() 
#selected an ARIMA(0,1,1) with drift, however with just the training data auto.arima() 
#selected an ARIMA(0,1,1) because the model does not have drift, the predictions stay as a flat line
# rather than having a trend.
residuals.3 <- test.wood3[1:10] - pred.wood3$mean[1:10]
sqrt(mean(residuals.3^2))
accuracy(forecast(fit.train3, h = 10), test.wood3)
#the Root Mean Squared Error for this model is 0.532. This value is less
#than the value for the wood2 model. This is a bit surprising as they both
#put a straight line on the graph for predictions. These values indicate that 
#the model for wood.3 was more accurate. The returns of fish that spent less time in fresh
#water and longer in the ocean were more accurately predicted than of the fish who spent
#two years in both freshwater and the ocean.
```

Wood 2.2
```{r}
pred.wood2 <- forecast(fit.train, h = 10)
plot(pred.wood2,ylab = "log(fish counted x1000)", xlab = "Time")
points(test.wood2, pch=19, col="red")
#the actual observations do fall within the 80% prediction interval
#This data is treated as white noise with the ARIMA(0,0,0) model so 
#I imagine the prediction intervals are tied very closely to the variance 
#of the residuals themselves.
residuals <- (test.wood2[1:10] - pred.wood2$mean[1:10])
RMSE.wood2 <- sqrt(mean(residuals^2))
accuracy(forecast(fit.train, h = 10), test.wood2)
#the RMSE done manually matches with the RMSE value that is found by the 
#model in row 2, column 2. This must be how the model does when compared to the 
#test data. So we have RMSE = 0.731
```

Kvichak 1.3
```{r}
#chooses same exact model as the full dataset model
pred.kvi3 <- forecast(fit.train.k3, h=10)
plot(pred.kvi3, ylab= "log(counted fish x1000", xlab ="Time")
points(test.kvi.3, pch=19, col ="red" ) 
resd.k3 <- test.kvi.3[1:10] - pred.kvi3$mean[1:10]
RMSE.k3 <- sqrt(mean(resd.k3^2))
accuracy(pred.kvi3, test.kvi.3)
#RMSE = 0.65 this time we see the prediction line with a positive trend
#this is because the model is ARIMA(0,1,1) with drift. less variance in
#this data set so the points fall much closer to the straight line prediction
#again we see that returns for fish that spent longer in the ocean were more accurately 
#predicted.
```


Kvichak 2.2
```{r}
pred.kvi2 <- forecast(fit.train.k2, h=10)
plot(pred.kvi2,ylab= "log(counted fish x1000", xlab ="Time")
points(test.kvi.2, pch=19, col ="red")
resd.k2 <- test.kvi.2[1:10] - pred.kvi2$mean[1:10]
RMSE.k2 <- sqrt(mean(resd.k2^2))
accuracy(pred.kvi2, test.kvi.2)
#manual calc matches accuracy(). RMSE = 2.088. points fell outside of the
#prediction intervals with this model. Same straight line provided by the MA() 
#model but there was more variance in this data which corresponds to the poor 
#prediction performance
```


# Discussion

Pull together results from two files

# Description of each team member's contributions



Example: "All team members helped decide on the goal and ran the analyses for the individual regions. Team members 2 & 3 wrote most of the code for the analysis of the regions. Team member 4 researched approaches for measuring accuracy of forecasts in [Hyndman & Athanasopoulos[OTexts.com/fpp2] and team member 2 added code for that to the methods. Team member 4 also researched tests for stationarity and worked with team member 2 to code that up. Team member 1 worked on the plotting section of the report using and adapting code that team member 3 wrote. All team members helped edit the report and wrote the discussion together."
