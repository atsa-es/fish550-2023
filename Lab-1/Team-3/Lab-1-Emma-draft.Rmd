---
title: "Emma-Lab-1-draft"
author: "Emma T-S"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data
We chose to work with all data to compare ability of the ARIMA models to forecast across different patterns of population dynamics and different sizes of training data sets.

# Question your team will address
1. If we approached these data as stock managers, how confident could we be in projected return data 5, 10, or 20 years into the future?
2. What types of population patterns are ARIMA models best suited for?
3. Is one species or region "easier" to forecast than others?

# What we actually did
It was harder to compare models across species and regions than we assumed. Using MASE as a metric helped, but it was still difficult to decipher why some regions were more easily forecasted and others returned unreliable results.

## Sockeye: Combined Regions
Read in data and load packages
```{r}
ruggerone_data <- readRDS(here::here("Lab-1", "Data_Images", "ruggerone_data.rds"))

library(tidyverse)
library(forecast)
```

Subset and format the data to analyze just sockeye.
```{r}
SockByRegion<-ruggerone_data %>%
  filter(region != "japan") %>%
  filter(region != "korea") %>%
  group_by(species, region, year) %>%
  summarize(total = sum(returns, na.rm=TRUE)) %>% 
  mutate(lnreturns = log(total)) %>%
  filter(species == "sockeye")
```

Create a time series object and train the data on the first 59 years of data; forecast the last 5 years.
```{r}
sockeye.ts<-ts(SockByRegion$lnreturns, start=SockByRegion$year[1])

train.sockeye<-window(sockeye.ts, start=1952, end=2010)
test.sockeye<-window(sockeye.ts, start=2011, end=2015)
```

Assess the ACF and PACF of the training data set.
```{r echo=F}
acf(train.sockeye)
pacf(train.sockeye)
```

Look at the options for fitting an ARIMA to the data and then choose a final model. The best model for both of these is ARIMA(0,1,2); however, a comparison with other models suggests that ARIMA(1,1,1) is also a good fit with AIC within 0.3 of the ARIMA(0,1,2). The full dataset requires differencing (d=1).
```{r}
fit <- forecast::auto.arima(train.sockeye, trace=T)
fit.final<-forecast::auto.arima(train.sockeye, approximation = F, stepwise = F)
```

Plot the 5 year forecast for the last part of the dataset compared to the actual data. The real data are represented by the black dots; the forecast is represented by the black line.
```{r echo=F}
fit.final %>%
  forecast(h=5) %>%
  autoplot() + geom_point(aes(x=x, y=y), data=fortify(test.sockeye))
```

## Assess how well forecasting performs for sockeye returns by region

Create objects needed for plotting.
```{r}
regions<-unique(SockByRegion$region)
regionskey<-c("Cook Inlet", "E. Kamchatka", "Kodiak", "Russia", "N.British Columbia",
              "Prince William Sound", "S. Alaska Pen.", "S. British Columbia", "SE Alaska", "W. Kamchatka", "Washington", "W. Alaska")
names(regionskey)<-regions
forecastlevels<-c(5, 10, 20)
Allcombs<-expand_grid(regions, forecastlevels)
```

Plot ACF and PACF for each region.
```{r results='hide'}
ACFandPACF<-function(reg){
  Sockdat<-SockByRegion %>% filter(region == reg)
  #create time series
  datts <- ts(Sockdat$lnreturns, start=Sockdat$year[1])
  return(list(a = acf(datts), p = pacf(datts)))
}
```
```{r}
DiagPlots<-lapply(regions, ACFandPACF)
names(DiagPlots)<-regions
```


ACF plots
```{r echo=F}
par(mfrow=c(3,4))
for(r in 1:length(regions)){
  plot(DiagPlots[[r]][[1]], main = paste0("Region: ", regionskey[r]))
}
```

PACF plots
```{r echo=F}
par(mfrow=c(3,4))
for(r in 1:length(regions)){
  plot(DiagPlots[[r]][[2]], main = paste0("Region: ", regionskey[r]))
}
```

Fit ARIMA models to each region.
```{r}
FitModFunction<-function(reg, forelevel){
  #filter region
  Sockdat<-SockByRegion %>% filter(region == reg)
  #create time series
  datts <- ts(Sockdat$lnreturns, start=Sockdat$year[1]) #this assumes the first year in data is the start of the time series (they are in order) 
  cutoff<-2015-forelevel
  train <- window(datts, 1952, cutoff)
  test <- window(datts, cutoff+1, 2015)
  
  mod <- auto.arima(train)
  mod
  
  res<-accuracy(forecast(mod, h=forelevel), test)[2,"MASE"] #test set MASE
  
  return(list(Fit = mod, MASE = res))
}

RegionMods<-mapply(FitModFunction, Allcombs$regions, Allcombs$forecastlevels, SIMPLIFY = FALSE)

```

Extract MASE for comparisons of models across regions.
```{r}
RegionMASE<-sapply(RegionMods, function(x){y<-x$MASE})
RegionBestMod<-sapply(RegionMods, function(x){y<-as.character(x$Fit)})

ResultsTable<-Allcombs %>% add_column(Model = RegionBestMod, MASE = RegionMASE)
```

Plot MASE for three different forecast periods - 5, 10, and 20 years - across all regions. MASE < 1 is a "good" value.
```{r echo=F}
ggplot(ResultsTable) + 
  geom_bar(aes(x = regions, y = MASE, fill = as.factor(forecastlevels)), stat = "identity", position = "dodge") + 
  geom_hline(aes(yintercept = 1), linetype = "dashed") + 
  scale_x_discrete(labels = as_labeller(regionskey)) +
  labs(fill = "Forecast Levels", x = "Region") + 
  ggtitle("Sockeye") + theme_bw() + theme(axis.text.x=element_text(angle=-90, hjust = 0, vjust = 0.5 ))


```

None of the ARIMA models performed well for the forecasts of 20 years of data. Below is a comparison of the three lengths of forecasted data for South British Columbia where MASE was below 1 for the 5 year forecast but >1 for the 10 and 20 year forecasts.

```{r}
sock.sbc<-subset(SockByRegion, region=='sbc')
sbc.ts<-ts(sock.sbc$lnreturns, start=sock.sbc$year[1])
#create training and test datasets for the 5, 10, and 20 year forecasts
train.sbc5<-window(sbc.ts, start=1952, end=2010)
test.sbc5<-window(sbc.ts, start=2011, end=2015)
sbc.final5<-forecast::auto.arima(train.sbc5, approximation = F, stepwise = F)

train.sbc10<-window(sbc.ts, start=1952, end=2005)
test.sbc10<-window(sbc.ts, start=2006, end=2015)
sbc.final10<-forecast::auto.arima(train.sbc10, approximation = F, stepwise = F)

train.sbc20<-window(sbc.ts, start=1952, end=1995)
test.sbc20<-window(sbc.ts, start=1996, end=2015)
sbc.final20<-forecast::auto.arima(train.sbc20, approximation = F, stepwise = F)
```


Here are plots of the three forecast scenarios for sockeye in South British Columbia.
```{r echo=F}
sbc.final5 %>%
  forecast(h=5) %>%
  autoplot() + geom_point(aes(x=x, y=y), data=fortify(test.sbc5))

sbc.final10 %>%
  forecast(h=10) %>%
  autoplot() + geom_point(aes(x=x, y=y), data=fortify(test.sbc10))

sbc.final20 %>%
  forecast(h=20) %>%
  autoplot() + geom_point(aes(x=x, y=y), data=fortify(test.sbc20))
```

How well does auto.arima do in choosing a model? Is it different from what we would choose looking at ACF and PACF?
For Cook Inlet, auto.arima selected ARIMA(4,1,1), but PACF has a significant lag at 6 and ACF trails off.
```{r}
sock.ci<-subset(SockByRegion, region=='ci')
ci.ts<-ts(sock.ci$lnreturns, start=sock.ci$year[1])
forecast::ndiffs(ci.ts, test='adf')
forecast::ndiffs(ci.ts, test='kpss')

train.ci5<-window(ci.ts, start=1952, end=2010)
test.ci5<-window(ci.ts, start=2011, end=2015)
ci.final5<-forecast::auto.arima(train.ci5, approximation = F, stepwise = F)
accuracy(forecast(ci.final5, h=5), test.ci5)
#MASE is just above 1

acf(train.ci5)
pacf(train.ci5)

#select ARIMA (6,1,0)
fit.ci2 <- Arima(train.ci5, order=c(6,1,0), include.mean=TRUE)
accuracy(forecast(fit, h=5), test.ci5)
#MASE is very high for the test set.
```

Here are the plots comparing the two models for Cook Inlet. The plots look very similar, but the forecast differs a bit for the last 2 years.
```{r echo=F}
ci.final5 %>%
  forecast(h=5) %>%
  autoplot() + geom_point(aes(x=x, y=y), data=fortify(test.ci5))

fit.ci2 %>%
  forecast(h=5) %>%
  autoplot() + geom_point(aes(x=x, y=y), data=fortify(test.ci5))
```


# Results
(Isn't the results section our plots and descriptions? I'm not sure how much detail to include.)

# Discussion
Comparing the models across the three forecasts for SBC underlines that the data chosen for a forecast matters. Not only is the estimated ARIMA a better fit for a shorter forecast (and is trained on more data), but the training data for the model for the 20 year forecast is not stationary so the ARIMA parameters are very different from the parameters estimated for the 5 and 10 year forecasts.
Six regions had at least one forecast with a well performing model (MASE < 1); however, six regions did not have MASE < 1 for even the 5 year forecast model. None of the 20 year forecasts resulted in MASE < 1. This suggests that our data are so stochastic that it is difficult to forecast more than 5 years into the future. Another hypothesis could be that the training data set isn't long enough to generate a good model for the 20 year forecast, but from looking at the data I think the large inter-year variability in returns makes it difficult to fit an accurate model.
In general, the models that fit the data better (lower MASE) tended to include differencing and were more likely to have higher order parameters.